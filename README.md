# LLMChat
Select any models of your choice within the configuration file, then make them chat!

# Example
Consider a 'google/flan-t5-base' vs 'google/flan-t5-base' chat. We chose this model because it is relatively small, and therefore can run on CPU in reasonable time. Suppose we query this small model on the meaning of life, here is the output conversation!
