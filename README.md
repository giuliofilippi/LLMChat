# LLMChat
Select any models of your choice within the configuration file, then make them chat!

# Example
Consider a "google/flan-t5-base" vs "MBZUAI/LaMini-Flan-T5-783M" chat. We chose these models because they are relatively small, and therefore can run locally on CPU in reasonable time. If you have GPU, feel free to load bigger models!
